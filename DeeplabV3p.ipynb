{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.layers import Layer, InputSpec, Conv2D, DepthwiseConv2D, UpSampling2D, ZeroPadding2D, Lambda, AveragePooling2D, Input, Activation, Concatenate, Add, Reshape, BatchNormalization, Dropout \n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "# from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "print(tf.__version__, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "[[7, 8, 9], [10, 11, 12]]\n",
      "tf.Tensor(\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]], shape=(4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = [[1, 2, 3], \n",
    "      [4, 5, 6]]\n",
    "t2 = [[7, 8, 9], \n",
    "      [10, 11, 12]]\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(tf.concat([t1,t2], 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# param\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "OUTPUT_CHANNELS = 2\n",
    "EPOCHS = 0\n",
    "away_from_computer = True  # to show or not predictions between batches\n",
    "save_model_for_inference = False # to save or not the model for inference\n",
    "\n",
    "# WEIGHTS_PATH_MOBILE = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5\"\n",
    "\n",
    "# dataset location\n",
    "Train_Images_Path = \"D:/Python/DataSets/ADE20K_Filtered/Train/Images/0/\"\n",
    "Val_Images_Path =  \"D:/Python/DataSets/ADE20K_Filtered/Validation/Images/0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     28,
     49,
     81
    ]
   },
   "outputs": [],
   "source": [
    "# similar to glob but with tensorflow\n",
    "train_imgs = tf.data.Dataset.list_files(Train_Images_Path + \"*.jpg\")\n",
    "val_imgs = tf.data.Dataset.list_files(Val_Images_Path + \"*.jpg\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def parse_image(img_path):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "    \n",
    "    mask_path = tf.strings.regex_replace(img_path, \"Images\", \"Masks\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \".jpg\", \"_seg.png\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "    \n",
    "    return {'image': image, 'segmentation_mask' : mask}\n",
    "\n",
    "train_set = train_imgs.map(parse_image)\n",
    "test_set = val_imgs.map(parse_image)\n",
    "dataset = {\"train\": train_set, \"test\": test_set}\n",
    "\n",
    "print(dataset.keys())\n",
    "\n",
    "\n",
    "# first create the function to normalize, resize and apply some data augmentation on the dataset:\n",
    "\n",
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of IMG_SIZE [IMG_SIZE,IMG_SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of IMG_SIZE [IMG_SIZE,IMG_SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_mask = tf.cast(input_mask, tf.uint8) / 255\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    \"\"\"Apply some transformations to an input dictionary\n",
    "    containing a train image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An annotation is a regular  channel image.\n",
    "    If a transformation such as rotation is applied to the image,\n",
    "    the same transformation has to be applied on the annotation also.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the test set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "# Then I set some parameters related to my dataset:\n",
    "\n",
    "train_imgs = glob(Train_Images_Path + \"*.jpg\")\n",
    "val_imgs = glob(Val_Images_Path + \"*.jpg\")\n",
    "TRAIN_LENGTH = len(train_imgs)\n",
    "VAL_LENGTH = len(val_imgs)\n",
    "print('train lenght: ', TRAIN_LENGTH)\n",
    "print('val lenght: ', VAL_LENGTH)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = BATCH_SIZE\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "print('steps per epoch: ', STEPS_PER_EPOCH)\n",
    "\n",
    "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = dataset['test'].map(load_image_test)\n",
    "\n",
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).cache().repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the Loaded Dataset\n",
    "\n",
    "def display_sample(display_list):\n",
    "    \"\"\"Show side-by-side an input image,\n",
    "    the ground truth and the prediction.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for image, mask in train.take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    # print(sample_mask)\n",
    "\n",
    "display_sample([sample_image, sample_mask])\n",
    "\n",
    "# print('train daset: ', train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:TF2_36] *",
   "language": "python",
   "name": "conda-env-TF2_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 907,
   "position": {
    "height": "642px",
    "left": "1557px",
    "right": "20px",
    "top": "124px",
    "width": "764px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
